{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying Convolutional Neural Networks for text classification using Keras since the API is straight forward.\n",
    "Word embeddings were all using GloVe (http://nlp.stanford.edu/data/glove.840B.300d.zip)\n",
    "\n",
    "First copied an example from Keras: https://github.com/keras-team/keras/blob/master/examples/imdb_cnn.py and got a Public Test score of 0.049.\n",
    "Then tried to emulate CNN-static from Yoon Kim (http://aclweb.org/anthology/D14-1181) and got a Public Test score of ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Activation, Conv1D, Embedding, Dense, Dropout, Flatten, GlobalMaxPooling1D, Input, MaxPooling1D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import regularizers\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "raw_comments = df.comment_text.values\n",
    "classes = df.drop(columns=['id', 'comment_text']).columns\n",
    "y = df[classes].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = 20000\n",
    "LEN_SENTENCE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_WORDS)\n",
    "tokenizer.fit_on_texts(list(raw_comments))\n",
    "tokens = tokenizer.texts_to_sequences(raw_comments)\n",
    "X = sequence.pad_sequences(tokens, maxlen=LEN_SENTENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.strip().split()) for o in open('glove.840B.300d.txt', 'r'))\n",
    "EMBED_SIZE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean = all_embs.mean()\n",
    "emb_std = all_embs.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "nb_words = min(MAX_WORDS, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, EMBED_SIZE))\n",
    "for word, i in word_index.items():\n",
    "    if i >= nb_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras example model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was based on the example from Keras. Surprisingly ended up performing better because other models overfitted more. The version of this that was submitted only used 1 epoch for training. As you can see from the output below, the log loss on the validation set increased on the second epoch despite lower training log loss, which indicates overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(LEN_SENTENCE,))\n",
    "x = Embedding(MAX_WORDS, EMBED_SIZE, weights=[embedding_matrix], trainable=True)(inp)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Conv1D(250, 3, padding='valid', activation='relu', strides=1)(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(250)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(len(classes), activation='sigmoid')(x)\n",
    "\n",
    "model_keras = Model(inputs=inp, outputs=x)\n",
    "model_keras.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 91058 samples, validate on 4793 samples\n",
      "Epoch 1/2\n",
      "91058/91058 [==============================] - 412s 5ms/step - loss: 0.0520 - acc: 0.9810 - val_loss: 0.0464 - val_acc: 0.9824\n",
      "Epoch 2/2\n",
      "91058/91058 [==============================] - 413s 5ms/step - loss: 0.0423 - acc: 0.9835 - val_loss: 0.0477 - val_acc: 0.9821\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4ae1b65e10>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_keras.fit(X, y, validation_split=0.05, batch_size=32, epochs=2, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks for Sentence Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CNN-static, CNN-nonstatic, and CNN-multichannel versions from the paper were implemented. These all performed significantly worse than the model above as you can see from the training and test metrics. A custom regularization method was implemented below based on the description in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FILTERS = 100\n",
    "conv_strides = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization on weights of penultimate layer: L2 norms cannot be higher than 3\n",
    "def custom_reg(weight_matrix):\n",
    "    return K.clip(K.l2_normalize(weight_matrix), float(\"-inf\"), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(LEN_SENTENCE,))\n",
    "x = Embedding(MAX_WORDS, EMBED_SIZE, weights=[embedding_matrix])(inp)\n",
    "\n",
    "convolutions = []\n",
    "for filter_window in [3, 4, 5]:\n",
    "    conv = Conv1D(NUM_FILTERS, filter_window, padding='valid', activation='relu', strides=conv_strides)(x)\n",
    "    pool_size = (LEN_SENTENCE - filter_window + 1) / conv_strides\n",
    "    conv = MaxPooling1D(pool_size=pool_size, strides=None)(conv)\n",
    "    conv = Flatten()(conv)\n",
    "    convolutions.append(conv)\n",
    "x = Concatenate()(convolutions)\n",
    "\n",
    "x = Dense(len(classes), activation='sigmoid', kernel_regularizer=custom_reg)(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "cnn_static = Model(inputs=inp, outputs=x)\n",
    "cnn_static.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 91058 samples, validate on 4793 samples\n",
      "Epoch 1/2\n",
      "91058/91058 [==============================] - 382s 4ms/step - loss: 0.3682 - acc: 0.9644 - val_loss: 0.1173 - val_acc: 0.9738\n",
      "Epoch 2/2\n",
      "91058/91058 [==============================] - 375s 4ms/step - loss: 0.3862 - acc: 0.9650 - val_loss: 0.1748 - val_acc: 0.9765\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4b287ee510>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_static.fit(X, y, validation_split=0.05, batch_size=50, epochs=2, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(LEN_SENTENCE,))\n",
    "x = Embedding(MAX_WORDS, EMBED_SIZE, weights=[embedding_matrix], trainable=True)(inp)\n",
    "\n",
    "convolutions = []\n",
    "for filter_window in [3, 4, 5]:\n",
    "    conv = Conv1D(NUM_FILTERS, filter_window, padding='valid', activation='relu', strides=conv_strides)(x)\n",
    "    pool_size = (LEN_SENTENCE - filter_window + 1) / conv_strides\n",
    "    conv = MaxPooling1D(pool_size=pool_size, strides=None)(conv)\n",
    "    conv = Flatten()(conv)\n",
    "    convolutions.append(conv)\n",
    "x = Concatenate()(convolutions)\n",
    "\n",
    "x = Dense(len(classes), activation='sigmoid', kernel_regularizer=custom_reg)(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "cnn_non_static = Model(inputs=inp, outputs=x)\n",
    "cnn_non_static.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 91058 samples, validate on 4793 samples\n",
      "Epoch 1/2\n",
      "91058/91058 [==============================] - 423s 5ms/step - loss: 0.3792 - acc: 0.9641 - val_loss: 0.1356 - val_acc: 0.9741\n",
      "Epoch 2/2\n",
      "91058/91058 [==============================] - 421s 5ms/step - loss: 0.4234 - acc: 0.9630 - val_loss: 0.4538 - val_acc: 0.9661\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4ae2383150>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_non_static.fit(X, y, validation_split=0.05, batch_size=50, epochs=2, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(LEN_SENTENCE,))\n",
    "\n",
    "channel_dynamic = Embedding(MAX_WORDS, EMBED_SIZE, weights=[embedding_matrix], trainable=True)(inp)\n",
    "channel_static = Embedding(MAX_WORDS, EMBED_SIZE, weights=[embedding_matrix])(inp)\n",
    "x = Concatenate()([channel_dynamic, channel_static])\n",
    "\n",
    "convolutions = []\n",
    "for filter_window in [3, 4, 5]:\n",
    "    conv = Conv1D(NUM_FILTERS, filter_window, padding='valid', activation='relu', strides=conv_strides)(x)\n",
    "    pool_size = (LEN_SENTENCE - filter_window + 1) / conv_strides\n",
    "    conv = MaxPooling1D(pool_size=pool_size, strides=None)(conv)\n",
    "    conv = Flatten()(conv)\n",
    "    convolutions.append(conv)\n",
    "x = Concatenate()(convolutions)\n",
    "\n",
    "x = Dense(len(classes), activation='sigmoid', kernel_regularizer=custom_reg)(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "cnn_multichannel = Model(inputs=inp, outputs=x)\n",
    "cnn_multichannel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 91058 samples, validate on 4793 samples\n",
      "Epoch 1/2\n",
      "91058/91058 [==============================] - 755s 8ms/step - loss: 0.4132 - acc: 0.9617 - val_loss: 0.2021 - val_acc: 0.9764\n",
      "Epoch 2/2\n",
      "91058/91058 [==============================] - 772s 8ms/step - loss: 0.3924 - acc: 0.9624 - val_loss: 0.1695 - val_acc: 0.9735\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4ae23efe10>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_multichannel.fit(X, y, validation_split=0.05, batch_size=50, epochs=2, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit on all training data, predict on test data, write to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "95851/95851 [==============================] - 474s 5ms/step - loss: 0.0556 - acc: 0.9801\n",
      "Epoch 2/2\n",
      "95851/95851 [==============================] - 461s 5ms/step - loss: 0.0419 - acc: 0.9836\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4af18ee890>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_keras.fit(X, y, batch_size=32, epochs=2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('test.csv')\n",
    "sub_tokens = tokenizer.texts_to_sequences(sub.comment_text.fillna(\"_na_\").values)\n",
    "X_sub = sequence.pad_sequences(sub_tokens, maxlen=LEN_SENTENCE)\n",
    "pred_sub = model.predict([X_sub], batch_size=1024, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = pd.concat([sub.drop(columns=['comment_text']), pd.DataFrame(pred_sub, columns=classes)], axis=1)\n",
    "df_sub.to_csv('conv_submission_keras_trainable_embedding_2epoch.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
